{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Machine Learning with PySpark and MLlib: Solving a Binary Classification Problem\n\nHere, we will learn how to build a **Binary Classification** application using **PySpark** and **MLlib Pipelines API**. \n\nWe tried **Logistic Regression**, **Decision Tree**, **Random Forest**, and **Gradient-Boosted Tree** algorithms and **Gradient Boosting Tree**  performed best on the data set."},{"metadata":{},"cell_type":"markdown","source":"**[Apache Spark](https://spark.apache.org/)**, once a component of the **[Hadoop](http://hadoop.apache.org/)** ecosystem, is now becoming the Big-Data platform of choice for enterprises. It is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Dataset\n\nWe will use the same data set when we built a [Logistic Regression](https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8/) in Python, and it is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict whether the client will subscribe (Yes/No) to a term deposit. The dataset can be downloaded from [Kaggle](https://www.kaggle.com/rouseguy/bankbalanced/data)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('ml-bank').getOrCreate()\nsdf = spark.read.csv('../input/bankbalanced/bank.csv', header = True, inferSchema = True)\nsdf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Input variables:** age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome\n\n**Output variable:** deposit"},{"metadata":{},"cell_type":"markdown","source":"Have a peek of the first five observations. Pandas data frame is prettier than Spark DataFrame show() method."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame(sdf.take(5), columns=sdf.columns).transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset classes are perfect balanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf.toPandas().groupby(['deposit']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary statistics for numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = [t[0] for t in sdf.dtypes if t[1] == 'int']\nsdf.select(numeric_features).describe().toPandas().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlations between independent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnumeric_data = sdf.select(numeric_features).toPandas()\naxs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\nn = len(numeric_data.columns)\n\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It’s obvious that there aren’t highly correlated numeric variables. Therefore, we will keep all of them for the model. \n\nHowever, day and month columns are not really useful, we will remove these two columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"sdf = sdf.select(\n    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', \n    'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit'\n)\ncols = sdf.columns\nsdf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Dataset for Machine Learning\n\nThe process includes **Category Indexing**, **One-Hot Encoding** and **VectorAssembler** (a feature transformer that merges multiple columns into a vector column).\n\nThis  code  indexes each categorical column using the <code>StringIndexer</code>, then converts the indexed categories into *one-hot encoded* variables. The resulting output has the binary vectors appended to the end of each row. We use the <code>StringIndexer</code> again to encode our labels to label indices. Next, we use the <code>VectorAssembler</code>  to combine all the feature columns into a single vector column."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n\nstages = []\ncategoricalColumns = [\n    'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome'\n]\n\nfor categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n    encoder = OneHotEncoderEstimator(\n        inputCols=[stringIndexer.getOutputCol()], \n        outputCols=[categoricalCol + \"classVec\"]\n    )\n    stages += [stringIndexer, encoder]\n    \nlabel_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\nstages += [label_stringIdx]\nnumericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\nassemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline\n\nWe use <code>Pipeline</code> to chain multiple Transformers and Estimators together to specify the **Machine Learning** workflow. A <code>Pipeline</code>’s stages are specified as an ordered array."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(sdf)\nsdf = pipelineModel.transform(sdf)\nselectedCols = ['label', 'features'] + cols\nsdf = sdf.select(selectedCols)\nsdf.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have <code>features</code> column and <code>label</code> column."},{"metadata":{"trusted":true},"cell_type":"code","source":"pdf = pd.DataFrame(sdf.take(5), columns=sdf.columns)\npdf.iloc[:,0:2] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pdf.features[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split dataset into train and test set\n\nRandomly split data into train and test sets, and set seed for reproducibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = sdf.randomSplit([0.7, 0.3], seed = 2018)\nprint(\"Training Dataset Count: \" + str(train.count()))\nprint(\"Test Dataset Count: \" + str(test.count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\nlrModel = lr.fit(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can obtain the coefficients by using <code>LogisticRegressionModel</code>’s attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta)\nplt.ylabel('Beta Coefficients')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summarize the model over the training set, we can also obtain the **ROC Receiver-Operating Characteristic)** and the **Area under ROC** (<code>areaUnderROC</code>)."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingSummary = lrModel.summary\nlrROC = trainingSummary.roc.toPandas()\n\nplt.plot(lrROC['FPR'],lrROC['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision and recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrPreds = lrModel.transform(test)\nlrPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrPreds.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nlrEval = BinaryClassificationEvaluator()\nprint('Test Area Under ROC', lrEval.evaluate(lrPreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier\n\n**Decision trees** are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\ndtModel = dt.fit(train)\ndtPreds = dtModel.transform(test)\ndtPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Decision Tree model\n\nOne simple decision tree performed poorly because it is too weak given the range of different features. The prediction accuracy of decision trees can be improved by Ensemble methods, such as Random Forest and Gradient-Boosted Tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtEval = BinaryClassificationEvaluator()\ndtROC = dtEval.evaluate(dtPreds, {dtEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(dtROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\nrf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\nrfModel = rf.fit(train)\nrfPreds = rfModel.transform(test)\nrfPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfEval = BinaryClassificationEvaluator()\nrfROC = rfEval.evaluate(rfPreds, {rfEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(rfROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient-Boosted Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(maxIter=10)\ngbtModel = gbt.fit(train)\ngbtPreds = gbtModel.transform(test)\ngbtPreds.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the Gradient-Boosted Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbtEval = BinaryClassificationEvaluator()\ngbtROC = gbtEval.evaluate(gbtPreds, {gbtEval.metricName: \"areaUnderROC\"})\nprint(\"Test Area Under ROC: \" + str(gbtROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient-Boosted Tree** achieved the best results, we will try tuning this model with the <code>ParamGridBuilder</code> and the <code>CrossValidator</code>. \n\nBefore that we can use <code>explainParams()</code> to print a list of all params and their definitions to understand what params available for tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(gbt.explainParams())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(gbt.maxDepth, [2, 4, 6])\n             .addGrid(gbt.maxBins, [20, 60])\n             .addGrid(gbt.maxIter, [10, 20])\n             .build())\n\ncv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=gbtEval, numFolds=5)\n\n# Run cross validations.  \n# This can take some minutes since it is training over 20 trees!\ncvModel = cv.fit(train)\ncvPreds = cvModel.transform(test)\ngbtEval.evaluate(cvPreds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}